{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"OpenInsiderScrape","provenance":[],"collapsed_sections":[],"mount_file_id":"1bNbBks2hCsNkHSl7NYhNS9uoPlHAGnvJ","authorship_tag":"ABX9TyNR+86heg3834nLUik0jLhu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"xGnGfArSlq1E"},"source":["Scraping the historical data of trades from http://openinsider.com/."]},{"cell_type":"markdown","metadata":{"id":"3xD0G1eul2Zg"},"source":["Setup and configs."]},{"cell_type":"code","metadata":{"id":"h0Cl9BvKdj8q","executionInfo":{"status":"ok","timestamp":1625804701837,"user_tz":240,"elapsed":38278,"user":{"displayName":"Cameron Erdman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjvtH2_-OGWkp1IUhnTHNQUweXaKBy-UvfJNM054GA=s64","userId":"08663479895544069007"}}},"source":["#Technically not python code but command line code. Pip is installing required packages and software\n","!pip install selenium &> /dev/null\n","!apt-get update &> /dev/null # to update ubuntu to correctly run apt install\n","!apt install chromium-chromedriver &> /dev/null # need freshest update of chromedriver\n","!cp /usr/lib/chromium-browser/chromedriver /usr/bin &> /dev/null"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"ftRmkf8Ajzrz","executionInfo":{"status":"ok","timestamp":1625804702027,"user_tz":240,"elapsed":197,"user":{"displayName":"Cameron Erdman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjvtH2_-OGWkp1IUhnTHNQUweXaKBy-UvfJNM054GA=s64","userId":"08663479895544069007"}}},"source":["import sys #sys gives the path to chromedriver so selenium can find it\n","sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n","from selenium import webdriver\n","from selenium.common.exceptions import NoSuchElementException \n","from selenium.webdriver.support.ui import Select      \n","import pandas as pd\n","import numpy as np\n","from datetime import datetime, timedelta #used to manipulate scrape by current day"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"xrVJwIm8jkOS","executionInfo":{"status":"ok","timestamp":1625804702029,"user_tz":240,"elapsed":21,"user":{"displayName":"Cameron Erdman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjvtH2_-OGWkp1IUhnTHNQUweXaKBy-UvfJNM054GA=s64","userId":"08663479895544069007"}}},"source":["chrome_options = webdriver.ChromeOptions()\n","chrome_options.add_argument('--headless') #In order for selenium to work on colab it must be headless\n","chrome_options.add_argument('--no-sandbox')\n","chrome_options.add_argument('--disable-dev-shm-usage')"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"0ZauOByejk7m"},"source":["#this allows you to connect to your own drive!\n","## this should work for anyone's drive who runs this code\n","import os\n","#Mount the drive from Google to save the dataset\n","from google.colab import drive # this will be our driver \n","drive.mount('/content/gdrive')\n","\n","# %load_ext google.colab.data_table       #makes pandas tables interactive              #Doesn't load in properly for some reason"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"04bfVV0tzJjS"},"source":["Initializes webdriver as wd, using chrome_options so that the webdriver doesn't crash in colab"]},{"cell_type":"code","metadata":{"id":"DTsInYRBc-2f"},"source":["wd = webdriver.Chrome('chromedriver',chrome_options=chrome_options)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DIg3XWMuzZqP"},"source":["Main body of scrape"]},{"cell_type":"markdown","metadata":{"id":"dhJewBG1QG1z"},"source":["Steps:\n","1.   Goes to [here\n","](http://openinsider.com/screener?s=&o=&pl=&ph=&ll=&lh=&fd=0&fdr=&td=0&tdr=&fdlyl=&fdlyh=&daysago=&xp=1&xs=1&vl=&vh=&ocl=&och=&sic1=-1&sicl=100&sich=9999&grp=0&nfl=&nfh=&nil=&nih=&nol=&noh=&v2l=&v2h=&oc2l=&oc2h=&sortcol=0&cnt=1000&page=1)\n","2.   puts in todays date in the filing date range\n","3.   Checks that num results exists and therefore isn't zero\n","4.   Scrapes for all wanted id's\n","5.   puts in next day\n","6.   repeats 3-5 unitl reaches target day\n","---\n","For anyone scraping in the future:\n","\n","Scraping using this method is very slow, I would advise splitting up your date range into 5 and making copies of this script in other colab files so as to run it 5 times faster.\n"]},{"cell_type":"markdown","metadata":{"id":"QvY1mujA02lk"},"source":["Function to find and populate a list for each elem in the table"]},{"cell_type":"code","metadata":{"id":"xZR3zQ8haGxm","executionInfo":{"status":"ok","timestamp":1625804721874,"user_tz":240,"elapsed":12,"user":{"displayName":"Cameron Erdman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjvtH2_-OGWkp1IUhnTHNQUweXaKBy-UvfJNM054GA=s64","userId":"08663479895544069007"}}},"source":["#For anyone looking at this code, this function is the reason it is slow. I am aware there exists faster methods to do it, I am just not skilled enough in python yet to discover them.\n","def find_elem(list_name, i):\n","  elements = wd.find_elements_by_css_selector(css[i])\n","  for elem in elements:\n","    list_name.append(elem.text)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a1zQ_5li08FV"},"source":["Function to check to make sure that the element exists"]},{"cell_type":"code","metadata":{"id":"8LPJ0ZPZF4sl","executionInfo":{"status":"ok","timestamp":1625804721876,"user_tz":240,"elapsed":12,"user":{"displayName":"Cameron Erdman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjvtH2_-OGWkp1IUhnTHNQUweXaKBy-UvfJNM054GA=s64","userId":"08663479895544069007"}}},"source":["def check_exists_by_css(css):\n","    try:\n","        wd.find_element_by_css_selector(css)\n","    except NoSuchElementException:\n","        return False\n","    return True\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"CWWV4Xc0oKjw","executionInfo":{"status":"ok","timestamp":1625804722147,"user_tz":240,"elapsed":281,"user":{"displayName":"Cameron Erdman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjvtH2_-OGWkp1IUhnTHNQUweXaKBy-UvfJNM054GA=s64","userId":"08663479895544069007"}}},"source":["def scrape_day(day, ids):\n","    wd.get(\"http://openinsider.com/screener?s=&o=&pl=&ph=&ll=&lh=&fd=0&fdr=&td=0&tdr=&fdlyl=&fdlyh=&daysago=&xp=1&xs=1&vl=&vh=&ocl=&och=&sic1=-1&sicl=100&sich=9999&grp=0&nfl=&nfh=&nil=&nih=&nol=&noh=&v2l=&v2h=&oc2l=&oc2h=&sortcol=0&cnt=1000&page=1\")\n","    custom = Select(wd.find_element_by_css_selector(\"#dropdownFD\")) # initializes the page as a custom date\n","    custom.select_by_value('-1')\n","\n","    date = (datetime.today() - timedelta(days = day)).strftime('%m/%d/%Y')\n","    input = wd.find_element_by_css_selector(\"#textboxFD\")\n","    input.send_keys(date + \" - \" + date)\n","    search = wd.find_element_by_css_selector(\"#grp2set+ table tr:nth-child(4) input\")\n","    search.submit()\n","\n","    #takes the element for css \"#results\" and splits it until the \" \" then grabs the first result which would be a number and casts it to int\n","    if check_exists_by_css(\"#results\"):\n","        for i in range(0,len(ids)):\n","          find_elem(ids[i],i)\n","\n","        trade_date.pop(0) # for some reason the css identifier fro trade_date has an extra \"\" at the beginning so we just need to pop it off so they're all the same length\n","        df = pd.DataFrame(np.column_stack([insider_name, titles, trade_type, tickers, prices, qty_purchased, values, number_owned, change_in_number_owned, filing_date, trade_date]),\n","                        columns = [\"Insider_Name\", \"Insider_Title\", \"Buy_or_Sell\", \"Ticker\", \"Price\", \"Quantity\", \"Value\", \"Number_Owned\", \"Change_in_Number_Owned\", \"Filing_Date\", \"Trade_Date\"])\n","      \n","        df.to_csv('/content/gdrive/MyDrive/OpenInsider/Saved_data/' + (datetime.today() - timedelta(days = day)).strftime('%m-%d-%Y') + '.csv') "],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ttdeRxTZ1FMs"},"source":["Main scrape function\n","\n","openInsider has data until 2003-07-28 08:52:51"]},{"cell_type":"code","metadata":{"id":"G6uYX-WWQGLp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625826899367,"user_tz":240,"elapsed":22177223,"user":{"displayName":"Cameron Erdman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjvtH2_-OGWkp1IUhnTHNQUweXaKBy-UvfJNM054GA=s64","userId":"08663479895544069007"}},"outputId":"c5e25735-33a1-4cad-ae75-a7188d29928a"},"source":["#setting up date to begin scraping from\n","day = 0\n","\n","#While loop to loop and scrape the day until the date being scraped is 7-28-2003\n","#Doing this so that if scrape fails on day 500 i dont loses all 500 days of data\n","while(timedelta(days = day) < (datetime.today() - datetime(2021, 7, 8))):\n","  prices = []\n","  values = []\n","  titles = []\n","  tickers = []\n","  trade_date = []  \n","  trade_type = [] \n","  filing_date = []\n","  insider_name = [] \n","  number_owned = []\n","  qty_purchased = []  \n","  change_in_number_owned = []\n","  ids = [tickers, prices, values, titles, trade_type, filing_date, trade_date, insider_name, qty_purchased, number_owned, change_in_number_owned]\n","  css = [\".tinytable td:nth-child(4) b a\", \"td:nth-child(9)\", \"td:nth-child(13)\", \"td:nth-child(7)\", \"td:nth-child(8)\", \".tinytable div a\", \"td~ td+ td div\", \"td:nth-child(6) a\", \"td:nth-child(10)\", \"td:nth-child(11)\", \"td:nth-child(12)\"]\n","  scrape_day(day, ids)\n","  day += 1\n","\n","print(day)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["6470\n"],"name":"stdout"}]}]}